%In the interest of time, we have to combine PCA and spectral analysis.
%We will focus on the what, why and how (in MATLAB). I'll have to owe you
%the mathematical underpinnings to really understand this (Linear Algebra
%and Fourier Analysis). But we'll see. I'll post primers.

%% 1 Time series
load handel %Load a preinstalled timeseries dataset
sound(y,0.5*Fs) %You can listen to any matrix

%Sound interprets the numbers in the matrix as a timeseries of amplitudes
%The numbers are interpreted as amplitudes of the speaker membrane
%deflections. We are whacking air molecules, which transfer the energy to
%other air molecules, by bumping into them. And we do this rapidly.
%Fs represents "sampling frequency". How often are the settings changed per second
%In measurement, this means how often is the signal recorded. 
%8192 is phone quality. 44100 is CD quality

%% b plotting it
figure
subplot(2,1,1)
plot(y)
title('Signal amplitude over time') %Technically over number of samples
shg
%This plot only shows us amplitude over time. It does not give us anything
%else. Importantly missing (as you see if you play it faster or slower) is 
%pitch. FREQUENCY of how fast it vibrates
%Most audio signals are generated by making some membrane vibrate.

%clear sound %Kills the sound

%Could be luminance over time. If you are living in this reality, signals
%come in as a function of time. 
%You can view the same data in "frequency space". The "fourier transform"
%is a transporter into that. It uses complex sine waves to decompose a
%signal into frequency components. 

%A spectrogram depicts the magnitude (technically power, magnitude^2 of the
%real part) of a frequency component of a signal (arbitrary signal) at a
%given time.
subplot(2,1,2)
windowShape = hanning(256); %Tradeoff in what windowing artifacts you get depending on shape
%Length of window determines tradeoff between knowing when frequency
%happened vs. what kind
frequencyRange = 0:64:Fs/2; %You can go up to nyquist, which is half sampling, but not more. This will go on the y-axis
overl = round(length(windowShape)/2); %Less overlap: Blocky. Little overlap: Slow (need to do a lot of FT)
spectrogram(y,windowShape,overl,frequencyRange,Fs,'yaxis')
colormap(jet)

%Long, blocky windows create a spectrogram fast, but it looks blocky/chunky

%% c Building up a complex signal from simple signals (pure sine waves)
samplingFrequency = 1e4; %10k
signalFreq1 = 440% Hz
t = 0:1/samplingFrequency:1; %Create a 1 second time base
signal1 = sin(2*pi*signalFreq1*t);
signalFreq2 = 523.25 %Awfully specific "C"
signal2 = sin(2*pi*signalFreq2*t); 
signal3 = signal1 + signal2; %Two tones at once from linear superposition of 2 sine waves
%% Look, listen and say
figure
plot(t,signal1)
sound(signal1,samplingFrequency)

%%
figure
subplot(2,1,1)
plot(t,signal3)
soundsc(signal3,samplingFrequency) %The dial tone!
%It is impossible for mere mortals to discern the elements of a signal that
%is made up of even just 2 pure tones, n noise, nothing. 
%Maybe we need to look at it in another way
%Visual inspection of the time domain signal fails
%Let's do a spectrogram instead to see what the same signal looks like in
%the frequency domain
subplot(2,1,2)
newWindow = hamming(2048);
newOverl = round(length(newWindow)/2);
newFrequencyRange = 0:5:800;
spectrogram(signal3,newWindow,newOverl,newFrequencyRange,samplingFrequency,'yaxis')

%% White noise (there are others, brown noise, pink noise)
wN = randn(samplingFrequency,1);
sound(wN,samplingFrequency)

%% 2 PCA

%% a) Init
clear all
close all
clc

%% b) Loader
%Numerical data
DATA = xlsread('/Users/lascap/Documents/Desktop as of August 2017/Old Desktop/Work/Teaching/Spring 2018/Programming Spring 2018/Programming Summer 2019/Course evaluation data.xlsx')
%DATA represent mean student response per course (in rows) and question
%(column), picked from courses with more than 30 students, so CLT kicks in
%Actual questions
[~, questions] = xlsread('/Users/lascap/Documents/Desktop as of August 2017/Old Desktop/Work/Teaching/Spring 2018/Programming Spring 2018/Programming Summer 2019/Evaluation questions.xlsx')
%If you know that an output will be empty, but you need the placeholder to
%get the 2nd output argument, you use ~
%Problem: NYU asks students to evaluate courses in 17 dimensions
%Result: a) Response rate is low
%b) Data is uninterpretable - how would you rank a course in 17 dimensions
%at once? We could do it if there was 1 question ("how good was the
%course")
%If we had a single variable, we could just sort the courses in that
%dimension
%We could just sum up all 17 and rank that. Problem: That over-weights
%things that are asked multiple times, but redundantly. We would like the
%data to tell us what is going on, not just reflect the biases of the
%committee who made the questions.
%Related to this: We could get separate scores for instructor and course,
%but this psychologically dubious. 

%% c) Look at the raw data!
%PCA is a way to highly process your data. 
%Processing can hide problems. Before you process the data, always look at
%it. Obviously, that is not doable with multivariate data (we just tried)
%so we need to visualize it
figure
imagesc(DATA); colormap(jet); colorbar;shg

%What this kind of raw data visualization reveals are problems, for
%instance reverse coding would show up as bands in a given column.
%Row-wise bands illustrate problems with a given course
%For instance, course 7 got amazing ratings throughout

%% d) Anything that the PCA can find is based on correlations, so we should be able to see this in the correlation matrix
figure
imagesc(corrcoef(DATA)); colormap(jet); colorbar;shg

%We can see analytical truths (that have to be true mathematically if we
%did this right) - you can use them as a manipulation check
%1) There needs to a "diagonal" of correlations with magnitude 1 - due to
%identity
%2) Symmetry - the "upper diagonal" contains the same correlations as the
%lower diagonal - the correlation between A and B is the same as between B
%and A (commutative)

%What about synthetic truths (that are true empirically), but not
%necessarily
%1) Overall, everything seems to ridiculously intercorrelated
%2) But one of these things does not seem like the others, specifically q10
%seems to be uncorrelated to everything else
%3) A second thing that is separate seems to be question 6 "is the course
%well organized - and that's it. 
%Importantly, students seem to be unable to discern quality of instructor
%from course quality. It's the same to them. 

%% e) Actually doing the PCA
%Beauty: This is 1 line in Matlab. It is an excruciating 6-step process
%that most people do only once per hand. You have to calculate the
%correlation matrix, extract the factors, rotate them, etc. 
%It gives us 3 outputs of interest: The "loadings", the data in the new
%coordinate system and the eigenvalues of the factors
%Loadings: How do the individual questions "load" on the factors (principal
%components). Loading: How much does the original vector point in the direction of the PC.
%You can think of it in terms of "how correlated is it" (that is not
%actually true, it is not a correlation, but I don't know what else to say
%without some serious linaer algebra).
%Eigenvalues: How much of the variance is explained by the factor
[loadings, origDataInNewDimensions, eigVal] = pca(DATA);

%We want to do PCA to reduce dimensions. But the PCA doesn't know when to
%stop. If you put 100 variables in, it will extract 100 factors. 
%But not all factors are created equal
%Not all factors explain an equal amount of variance
%The first one explains the most (by design), the 2nd less, and so on.
varExplained = eigVal./sum(eigVal).*100 %Variance explained by the factor

%% f The "Scree plot"

%Just to show you that the PCA does all of these steps at once, let's do
%one de novo
eigenValMag = eig(corrcoef(DATA)); %Redoing just the extraction of the eigenvalues from the correlation matrix
figure
bar(1:length(eigenValMag),sortrows(eigenValMag,-1))
title('Scree plot')
line([min(xlim) max(xlim)],[1 1],'linestyle','--')
%The Eigensum or Eigenmass corresponds to the number of questions. Each
%variable adds 1. The reason we mention this is because it leads to one
%principle of factor retention: The Kaiser criterion. We only keep
%eigenvalues that are larger than 1. They have to earn their keep.
%You could also say - by visual inspection that there are 1 or 3 factors,
%by the "elbow" criterion in the screeplot. 
%Horn's method (in my 2nd book) - the most principled way

%% Coming up: What to name the factors (by looking at loadings)
%How to use this for decision making - looking at the old data in the new
%coordinate system

